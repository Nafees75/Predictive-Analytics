{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Default Prediction Analysis\n",
    "\n",
    "\n",
    "This analysis aims to predict credit card default using a dataset of credit card holders. The dataset contains various features such as demographic details, payment history, and bill statements. The primary goal is to build a robust model that can predict whether a customer will default on their credit card payment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Dataset\n",
    "\n",
    "The dataset contains information about credit card holders and includes the following columns:\n",
    "\n",
    "- LIMIT_BAL: Amount of given credit\n",
    "- SEX: Gender (1 = male, 2 = female)\n",
    "- EDUCATION: Education level\n",
    "- MARRIAGE: Marital status\n",
    "- AGE: Age in years\n",
    "- PAY_0 to PAY_6: Repayment status in months 0 to 6\n",
    "- BILL_AMT1 to BILL_AMT6: Bill statement amounts in months 1 to 6\n",
    "- PAY_AMT1 to PAY_AMT6: Previous payments in months 1 to 6\n",
    "- default payment next month: Default payment (1 = default, 0 = no default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and delete ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_excel('credit_data.xlsx')\n",
    "\n",
    "# Read the first 10,000 rows and delete the 'ID' column\n",
    "df = df.head(10000).drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After loading the dataset into the notebook, we can further check basic information of the dataset such as data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   LIMIT_BAL                   10000 non-null  int64  \n",
      " 1   SEX                         9617 non-null   float64\n",
      " 2   EDUCATION                   9617 non-null   float64\n",
      " 3   MARRIAGE                    9617 non-null   float64\n",
      " 4   AGE                         9617 non-null   float64\n",
      " 5   PAY_0                       9617 non-null   float64\n",
      " 6   PAY_2                       9617 non-null   float64\n",
      " 7   PAY_3                       9617 non-null   float64\n",
      " 8   PAY_4                       9642 non-null   float64\n",
      " 9   PAY_5                       9642 non-null   float64\n",
      " 10  PAY_6                       9642 non-null   float64\n",
      " 11  BILL_AMT1                   9642 non-null   float64\n",
      " 12  BILL_AMT2                   9642 non-null   float64\n",
      " 13  BILL_AMT3                   9642 non-null   float64\n",
      " 14  BILL_AMT4                   9640 non-null   float64\n",
      " 15  BILL_AMT5                   9640 non-null   float64\n",
      " 16  BILL_AMT6                   9640 non-null   float64\n",
      " 17  PAY_AMT1                    9640 non-null   float64\n",
      " 18  PAY_AMT2                    9640 non-null   float64\n",
      " 19  PAY_AMT3                    9998 non-null   float64\n",
      " 20  PAY_AMT4                    9998 non-null   float64\n",
      " 21  PAY_AMT5                    9700 non-null   float64\n",
      " 22  PAY_AMT6                    9700 non-null   float64\n",
      " 23  default payment next month  10000 non-null  int64  \n",
      "dtypes: float64(22), int64(2)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Definitions of the three kinds of variables\n",
    "- Numeric Variables: Numeric variables can take on a range of numerical values, either discrete or continuous. They are often used in mathematical models and statistical analyses.\n",
    "- Ordinal Variables: Ordinal variables represent categories with a specific order or ranking. The order of these values is important, but the difference between each one is not necessarily known.\n",
    "- Nominal Variables: Nominal variables represent categorical data without a specific order or ranking. They do not have a numerical value and cannot be ordered or ranked.\n",
    "\n",
    "Based on the provided output of the data info, here's the breakdown of features into numeric, ordinal, and nominal variables:\n",
    "\n",
    "| Variable Kind | Number of Features | Feature Names |\n",
    "| --- | --- | --- |\n",
    "| Numeric | 14 | LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, |\n",
    "| Ordinal | 7 | PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, EDUCATION |\n",
    "| Nominal | 2 | SEX, MARRIAGE |\n",
    "\n",
    "##### Explanation:\n",
    "\n",
    "- Numeric Variables (14 features): These are features with numerical values. They include LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6. For example, 'LIMIT_BAL' represents the credit limit of cardholders. Each entry in the dataset can be expressed as numerical values. \n",
    "\n",
    "- Ordinal Variables (7 features): These features represent categorical data with a specific order. They include PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6 and EDUCATION. These variables have numerical values indicating different levels of payment delays and EDUCATION levels. Example from the dataset, PAY_0 to PAY_6 represent the repayment status of the account holder over several months. These variables indicates different levels of payment delays with higher values indicating more severe delays. For education, 1.0 may refer to high school, University might be 2.0 and graduate school can be 3.0 which is really difficul to determine.\n",
    "\n",
    "- Nominal Variables (2 features): These features represent categorical data without any specific order. They include SEX (gender) and MARRIAGE (marital status). These variables do not have numerical values and cannot be ordered or ranked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset and Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values for each variable:\n",
      "LIMIT_BAL                       0\n",
      "SEX                           383\n",
      "EDUCATION                     383\n",
      "MARRIAGE                      383\n",
      "AGE                           383\n",
      "PAY_0                         383\n",
      "PAY_2                         383\n",
      "PAY_3                         383\n",
      "PAY_4                         358\n",
      "PAY_5                         358\n",
      "PAY_6                         358\n",
      "BILL_AMT1                     358\n",
      "BILL_AMT2                     358\n",
      "BILL_AMT3                     358\n",
      "BILL_AMT4                     360\n",
      "BILL_AMT5                     360\n",
      "BILL_AMT6                     360\n",
      "PAY_AMT1                      360\n",
      "PAY_AMT2                      360\n",
      "PAY_AMT3                        2\n",
      "PAY_AMT4                        2\n",
      "PAY_AMT5                      300\n",
      "PAY_AMT6                      300\n",
      "default payment next month      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values for each variable\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# Print out the number of missing values for each variable\n",
    "print(\"Number of missing values for each variable:\")\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Complete Data:\n",
    "- The variables LIMIT_BAL and default payment next month have no missing values, indicating that the dataset is complete for these variables.\n",
    "\n",
    "#### Missing Values:\n",
    "- PAY_AMT3 and PAY_AMT4 have only 2 missing values.\n",
    "- PAY_AMT5 and PAY_AMT6 have 300 missing values.\n",
    "- PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, and BILL_AMT3 have 358 missing values.\n",
    "- BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, and PAY_AMT2 have 360 missing values.\n",
    "- SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, and PAY_3 have 383 missing values.\n",
    "\n",
    "\n",
    "Handling missing data is very essential to ensure the validity of any analysis. Some of the common strategies are: imputation (replacing missing values with statistical estimates) and deletion (excluding missing values from analysis). The appropriate method depends on the nature of the missing data and the specific analysis being conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define and Impute missing values for numeric variables with mean\n",
    "numeric_columns = ['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', \n",
    "                   'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', \n",
    "                   'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n",
    "\n",
    "# Define and Impute missing values for categorical variables with mode\n",
    "categorical_columns = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Check for missing values after new imputation\n",
    "missing_values_after_imputation = df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation Explanation:\n",
    "\n",
    "Data imputation is the process of filling in missing values in a dataset with estimated or substituted values. It is a crucial step in data preprocessing, as missing data can adversely affect the performance of machine learning algorithms.\n",
    "\n",
    "#### Imputation Decisions:\n",
    "- Numeric Variables: For numeric variables (LIMIT_BAL, AGE, PAY_0 to PAY_6, BILL_AMT1 to BILL_AMT6, PAY_AMT1 to PAY_AMT6), missing values are imputed using the mean of the respective column. Imputing the mean for missing values is the best solution because this will not change the overall distribution of the dataset and it is considered absent at random.\n",
    "- Categorical Variables: For categorical variables (SEX, EDUCATION, MARRIAGE), missing values are imputed using the mode (most frequent value) of the respective column. Imputing with the mode is appropriate for imputing categorical values, as it replaces missing values with the most commonly appearing category and hence it does not change the distribution of the variable.\n",
    "\n",
    "#### Imputation Methods Used:\n",
    "- Mean Imputation: The mean imputation is one technique to calculate missing values. Some characteristics of mean imputation include, For numerical variables, the mean value of the corresponding column substitutes for the missing values. Mean imputation can be done quickly and is very simple. One disadvantage of mean imputation is that, regardless of the missing data being completely random, it changes the distribution of the original data.\n",
    "- Mode Imputation: The mode imputation substitutes the most common category for the missing values. In categorical data. Like mean imputation, if missing data is not completely at random. Mode imputation could affect the original distribution of the data.\n",
    "\n",
    "#### Key Decisions Made:\n",
    "- Selecting the Imputation Method: It is one of the technique that was prevented to choose the most appropriate method for each type of data; the selection of the imputation method was depends on the column was numeric or categorical.\n",
    "- Assumptions Regarding Missing Data: TThe only necessary assumption required for these imputations is that the data are missing completely at random (MCAR). The imputation can cause bias if this assumption is false. \n",
    "- Not Eliminating Any Observations: To retain the integrity of the analysis and preserve the size of the dataset, observations with missing values were decided to be imputed rather than deleted, which might result in data loss.\n",
    "\n",
    "By effectively imputing missing values using suitable methods, the dataset maintains its completeness and allows for reliable analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts of 'SEX' column:\n",
      "2.0    6206\n",
      "1.0    3794\n",
      "Name: SEX, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print value_counts() of the 'SEX' column\n",
    "print(\"Value counts of 'SEX' column:\")\n",
    "print(df['SEX'].value_counts())\n",
    "\n",
    "# Add dummy variable 'SEX_FEMALE' to df using get_dummies()\n",
    "df = pd.get_dummies(df, columns=['SEX'], prefix='SEX', drop_first=True)\n",
    "\n",
    "# Rename SEX columns\n",
    "df.rename(columns={'SEX_2.0': 'SEX_FEMALE'}, inplace=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Reorder columns\n",
    "limit_bal_index = df.columns.get_loc('LIMIT_BAL')\n",
    "\n",
    "# Insert the 'SEX_FEMALE' column after the 'LIMIT_BAL' column\n",
    "df.insert(limit_bal_index + 1, 'SEX_FEMALE', df.pop('SEX_FEMALE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counts of 'SEX' Column:\n",
    "\n",
    "The value_counts() function is used to display the frequency of unique values in the 'SEX' column. In this case, the output shows that there are 6206 occurrences of the value 2.0 (presumably representing females) and 3794 occurrences of the value 1.0 (presumably representing not females).\n",
    "\n",
    "#### Dummy Variable 'SEX_FEMALE':\n",
    "A dummy variable named 'SEX_FEMALE' is created using the get_dummies() function. This new variable indicates whether the individual is female or not.\n",
    "- If 'SEX_FEMALE' is 1 or True, it means the individual is female.\n",
    "- If 'SEX_FEMALE' is 0 or False, it means the individual is not female.\n",
    "\n",
    "#### Deletion of 'SEX' Column:\n",
    "After creating the dummy variable 'SEX_FEMALE' using the get_dummies() function from the Pandas library in order to execute out this change. In order to prevent repetition, the original \"SEX\" column has been deleted from the dataset.\n",
    "\n",
    "\n",
    "##### Additional Steps:\n",
    "- The drop_first=True parameter in get_dummies() drops the first level of each categorical variable to prevent multicollinearity in modeling.\n",
    "- The dummy variable 'SEX_FEMALE' is inserted immediately after the 'LIMIT_BAL' column for organizational purposes, using the insert() function.\n",
    "\n",
    "Overall, these steps transform the categorical 'SEX' variable into a binary dummy variable 'SEX_FEMALE', facilitating its use in subsequent analyses or modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts of 'MARRIAGE' column:\n",
      "2.0    5511\n",
      "1.0    4361\n",
      "3.0     110\n",
      "0.0      18\n",
      "Name: MARRIAGE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print value_counts() of the 'MARRIAGE' column\n",
    "print(\"Value counts of 'MARRIAGE' column:\")\n",
    "print(df['MARRIAGE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on 'MARRIAGE' Variable:\n",
    "The 'MARRIAGE' variable indicates marital status. Key observations from the value_counts() output are as follows:\n",
    "\n",
    "#### a) Categories 1 (Married) and 2 (Single):\n",
    "- Category 1 (Married) has 4361 occurrences and Category 2 (Single) has 5511 occurrences.\n",
    "- These categories are the most common marital statuses in the dataset.\n",
    "\n",
    "#### b) Category 3 (Others):\n",
    "- Category 3 has 110 occurrences.\n",
    "- This category likely represents other marital statuses.\n",
    "\n",
    "#### c) Category 0 (Missing/Undefined):\n",
    "- Category 0 has 18 occurrences.\n",
    "- This category is rare and likely indicates missing or undefined values.\n",
    "\n",
    "To ensure accurate modelling and reliable insights, it is important to evaluate the distribution and interpretation of categorical variables such as 'MARRIAGE'. Understanding the distribution of marital status facilitates the development of prediction models capable of accurately projecting future events such as loan default or spending habits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping and Encoding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mapping numerical values to corresponding categories in 'MARRIAGE' column\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace({\n",
    "    2.0: 'MARRIAGE_MARRIED',\n",
    "    1.0: 'MARRIAGE_SINGLE',\n",
    "    3.0: 'MARRIAGE_OTHER',\n",
    "    0.0: 'MARRIAGE_OTHER'   # assume 0 represents 'MARRIAGE_OTHER'\n",
    "})\n",
    "\n",
    "# Add dummy variable 'MARRIAGE' to df using get_dummies()\n",
    "df = pd.get_dummies(df, columns=['MARRIAGE'])\n",
    "\n",
    "# Reorder columns\n",
    "education_index = df.columns.get_loc('EDUCATION')\n",
    "\n",
    "# Insert the 'MARRIAGE' columns after the 'EDUCATION' column\n",
    "df.insert(education_index + 1, 'MARRIAGE_MARRIED', df['MARRIAGE_MARRIAGE_MARRIED'])\n",
    "df.insert(education_index + 2, 'MARRIAGE_OTHER', df['MARRIAGE_MARRIAGE_OTHER'])\n",
    "df.insert(education_index + 3, 'MARRIAGE_SINGLE', df['MARRIAGE_MARRIAGE_SINGLE'])\n",
    "\n",
    "# Drop the original 'MARRIAGE' columns\n",
    "df.drop(columns=['MARRIAGE_MARRIAGE_MARRIED', 'MARRIAGE_MARRIAGE_OTHER', 'MARRIAGE_MARRIAGE_SINGLE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocation of 'MARRIAGE' Values Across Dummy Variables:\n",
    "To ensure appropriate representation and interpretation, many judgements were taken while assigning values of the 'MARRIAGE' variable over the three newly constructed features ('MARRIAGE_MARRIED, 'MARRIAGE_SINGLE, and 'MARRIAGE_OTHER').\n",
    "\n",
    "#### 1.\tMapping:\n",
    "\n",
    "The original numerical values were mapped to corresponding categories as follows:\n",
    "-  2.000000 was mapped to 'MARRIAGE_MARRIED', representing married individuals.\n",
    "-  1.000000 was mapped to 'MARRIAGE_SINGLE', representing single individuals.\n",
    "-  Both 3.000000 and 0.000000 were mapped to 'MARRIAGE_OTHER', representing other marital statuses or potentially missing or undefined values.\n",
    "\n",
    "#### 2.\tEncoding Method:\n",
    "- For each category, binary features were created using one-hot encoding. By precisely assigning a value of 1 to each observation in the associated feature and 0 to others, this strategy makes it easier to do additional analysis and modelling.\n",
    "\n",
    "#### 3.\tHandling Missing/Undefined Values:\n",
    "- Although 'MARRIAGE_OTHER' is assumed to be represented by 0.000000, more analysis is required to verify whether or not this value actually represents missing or undefined values. By doing this, it is ensured that the data is correctly interpreted and displayed, preventing any possibility of error.\n",
    "\n",
    "#### 4.\tInterpretability:\n",
    "- Readability is improved by keeping relevant category labels, such as \"MARRIAGE_MARRIED,\" \"MARRIAGE_SINGLE,\" and \"MARRIAGE_OTHER.\" Evaluating model predictions and data-driven insights is facilitated by well-defined labels.\n",
    "\n",
    "By carefully considering these factors during the allocation process, the resulting dummy variables accurately represent the marital status of individuals in the dataset, laying the foundation for reliable analysis and modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the column 'EDUCATION', convert the values {0, 5, 6} to the value 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values {0, 5, 6} in the 'EDUCATION' column with 4\n",
    "df['EDUCATION'] = df['EDUCATION'].replace({0: 4, 5: 4, 6: 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "**Splitting the Data**\n",
    "\n",
    "We split the data into training and testing sets, with 75% of the data used for training and 25% for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Rename the column 'default payment next month' to 'default payment'\n",
    "df = df.rename(columns={'default payment next month': 'payment_default'})\n",
    "\n",
    "# Extract the target variable 'payment_default' for the first 7,500 observations\n",
    "y = df['payment_default'].values[:7500]\n",
    "\n",
    "# Extract the features (all remaining variables except 'default payment next month') for the first 7,500 observations\n",
    "X = df.drop(columns=['payment_default']).values[:7500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset accuracy: 0.8234666666666667\n",
      "Test dataset accuracy: 0.8138666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a Support Vector Classifier with RBF kernel\n",
    "svc_classifier = SVC(kernel='rbf', random_state=24)\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions on training and test datasets\n",
    "y_train_pred = svc_classifier.predict(X_train_scaled)\n",
    "y_test_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Compute and print training and test dataset accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training dataset accuracy:\", train_accuracy)\n",
    "print(\"Test dataset accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset accuracy (with PCA): 0.8017777777777778\n",
      "Test dataset accuracy (with PCA): 0.8037333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on standardized features and transform them to obtain the principal components\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train a Support Vector Classifier with RBF kernel on the principal components\n",
    "svc_classifier_pca = SVC(kernel='rbf', random_state=24)\n",
    "svc_classifier_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions on training and test datasets\n",
    "y_train_pred_pca = svc_classifier_pca.predict(X_train_pca)\n",
    "y_test_pred_pca = svc_classifier_pca.predict(X_test_pca)\n",
    "\n",
    "# Compute and print training and test dataset accuracies\n",
    "train_accuracy_pca = accuracy_score(y_train, y_train_pred_pca)\n",
    "test_accuracy_pca = accuracy_score(y_test, y_test_pred_pca)\n",
    "\n",
    "print(\"Training dataset accuracy (with PCA):\", train_accuracy_pca)\n",
    "print(\"Test dataset accuracy (with PCA):\", test_accuracy_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparing the two classifiers trained on the Credit Card Defaults data, the following observations can be made:\n",
    "\n",
    "#### 1.\tSupport Vector Classifier on Standardized Data:\n",
    "- Achieved a training accuracy of 0.8234666666666667 and a test accuracy of 0.8138666666666666 which indicates good generalization ability as the test accuracy is close to the training accuracy which means that the model effectively performed.\n",
    "\n",
    "#### 2.\tSupport Vector Classifier on Principal Components:\n",
    "- Trained on two main components derived from the standardized features that achieved lower training accuracy of 0.8017777777777778 and a test accuracy of 0.8037333333333333 which means that it sacrifices some interpretability by transforming features into a lower-dimensional space.\n",
    "\n",
    "#### 3.\tComparison:\n",
    "- When compared to the SVC with main components, the SVC with standard features performs better on both training and testing datasets.\n",
    "\n",
    "- However, the slight variation in accuracy raises the possibility that some important data was overlooked during PCA's dimensionality reduction procedure.\n",
    "\n",
    "- The original features have a greater predictive power than the reduced main components, as seen by the better accuracy of the SVC with standard features.\n",
    "\n",
    "#### 4.\tConclusion:\n",
    "These data suggest that, since the performance of the two classifiers is comparable, the Support Vector Classifier that was trained using all of the standardised characteristics would be more efficient to forecast credit card defaults in this particular situation. Although both classifiers perform reasonably well, the choice between them depends on factors such as the importance of interpretability, computational efficiency, and the specific requirements of the application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
